{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYD7ILB2MJF8",
        "outputId": "a658c9ea-38b6-4f0c-fc1e-21683086fb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=89da142953e24304a035cb0fb5fa593cfc9d6ceb69b64b7a70eaa9509a586204\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv\n",
        "! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv\n",
        "spark = SparkSession.builder.appName(\"Credit\").getOrCreate()\n",
        "df_data = spark.read.csv(\"gcs_internal_data.csv\", header=True, inferSchema=True)\n",
        "df_ext = spark.read.csv(\"gcs_external_sources.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df_full = df_data.join(df_ext, on='SK_ID_CURR', how='inner')\n",
        "\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "df = df_full.select(columns_extract)\n",
        "\n",
        "seed = 101\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=seed)\n",
        "\n",
        "categorical_columns = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
        "\n",
        "for indexer in indexers:\n",
        "    train = indexer.fit(train).transform(train)\n",
        "    test = indexer.fit(test).transform(test)\n",
        "\n",
        "encoder = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", dropLast=False) for col in categorical_columns]\n",
        "\n",
        "for enc in encoder:\n",
        "    train = enc.fit(train).transform(train)\n",
        "    test = enc.fit(test).transform(test)\n",
        "\n",
        "input_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "              'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH',\n",
        "              'AMT_ANNUITY', 'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "              'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE'] + \\\n",
        "             [col + \"_encoded\" for col in categorical_columns]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "train = assembler.setParams(handleInvalid=\"skip\").transform(train)\n",
        "test = assembler.setParams(handleInvalid=\"skip\").transform(test)\n",
        "\n",
        "train = train.select(\"TARGET\", \"features\")\n",
        "test = test.select(\"TARGET\", \"features\")\n",
        "\n",
        "random_forest = RandomForestClassifier(numTrees=100, featuresCol=\"features\", labelCol=\"TARGET\", seed=50)\n",
        "model = random_forest.fit(train)\n",
        "predictions = model.transform(test)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"TARGET\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", (accuracy*100))\n",
        "\n",
        "model.save(\"random_forest_model\")\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbCXV3FY2h_5",
        "outputId": "503bac3a-ee17-414a-a79e-d5c7aef24fae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 21:11:18--  https://storage.googleapis.com/bdt-spark-store/external_sources.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.146.207, 209.85.147.207, 142.250.125.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.146.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15503836 (15M) [text/csv]\n",
            "Saving to: ‘gcs_external_sources.csv’\n",
            "\n",
            "gcs_external_source 100%[===================>]  14.79M  14.0MB/s    in 1.1s    \n",
            "\n",
            "2023-11-06 21:11:19 (14.0 MB/s) - ‘gcs_external_sources.csv’ saved [15503836/15503836]\n",
            "\n",
            "--2023-11-06 21:11:19--  https://storage.googleapis.com/bdt-spark-store/internal_data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.146.207, 209.85.147.207, 142.250.125.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.146.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152978396 (146M) [text/csv]\n",
            "Saving to: ‘gcs_internal_data.csv’\n",
            "\n",
            "gcs_internal_data.c 100%[===================>] 145.89M  32.9MB/s    in 5.0s    \n",
            "\n",
            "2023-11-06 21:11:25 (29.3 MB/s) - ‘gcs_internal_data.csv’ saved [152978396/152978396]\n",
            "\n",
            "Accuracy: 93.24307692307693\n"
          ]
        }
      ]
    }
  ]
}